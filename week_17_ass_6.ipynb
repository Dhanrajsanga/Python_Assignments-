{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "183a0186-49ee-4d10-b9ab-0724d79bd0d9",
   "metadata": {},
   "source": [
    "## 1.You are working on a machine learning project where you have a dataset containing numerical and categorical features. You have identified that some of the features are highly correlated, and there are missing values in some of the columns. You want to build a pipeline that automates the feature engineering process and handles the missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1da255-1083-4f96-81df-460256f734e9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "Design a pipeline that includes the following steps:\n",
    "\n",
    "1. Use an automated feature selection method to identify the important features in the dataset.\n",
    "2. Create a numerical pipeline that includes the following steps:\n",
    "   - Impute the missing values in the numerical columns using the mean of the column values.\n",
    "   - Scale the numerical columns using standardization.\n",
    "3. Create a categorical pipeline that includes the following steps:\n",
    "   - Impute the missing values in the categorical columns using the most frequent value of the column.\n",
    "   - One-hot encode the categorical columns.\n",
    "4. Combine the numerical and categorical pipelines using a ColumnTransformer.\n",
    "5. Use a Random Forest Classifier to build the final model.\n",
    "6. Evaluate the accuracy of the model on the test dataset.\n",
    "\n",
    "Note: Your solution should include code snippets for each step of the pipeline and a brief explanation of each step. You should also provide an interpretation of the results and suggest possible improvements for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fa9b7-c0e2-487c-bd1c-ba4755b1b486",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "Explanation:\n",
    "1. **Automated feature selection**: SelectFromModel with RandomForestClassifier is used to automatically select important features based on their importance scores.\n",
    "2. **Numerical pipeline**: Impute missing values with the mean and scale numerical columns using standardization.\n",
    "3. **Categorical pipeline**: Impute missing values with the most frequent value and one-hot encode categorical columns.\n",
    "4. **Combining pipelines**: ColumnTransformer is used to combine the numerical and categorical pipelines.\n",
    "5. **Final pipeline**: Includes preprocessing steps, feature selection, and a Random Forest Classifier.\n",
    "6. **Model evaluation**: The accuracy of the model is evaluated on the test dataset.\n",
    "   \n",
    "Improvements:\n",
    "- Experiment with different imputation strategies and scaling techniques to find the most suitable ones for your dataset.\n",
    "- Consider using more sophisticated feature selection methods or hyperparameter tuning for better model performance.\n",
    "- Explore other classification algorithms besides Random Forest to compare performance and choose the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7aefbca2-df98-4482-a86e-668808df6ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2643163267352766\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "# Load the tips dataset from Seaborn\n",
    "tips_df = sns.load_dataset(\"tips\")\n",
    "\n",
    "# Define numerical and categorical columns\n",
    "numerical_columns = ['total_bill', 'size']\n",
    "categorical_columns = ['sex', 'smoker', 'day', 'time']\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = tips_df.drop(columns=['tip'])\n",
    "y = tips_df['tip']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Automated feature selection method to identify important features\n",
    "feature_selection = SelectFromModel(RandomForestRegressor())\n",
    "\n",
    "# Numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
    "    ('scaler', StandardScaler())  # Scale numerical columns using standardization\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with most frequent value\n",
    "    ('encoder', OneHotEncoder())  # One-hot encode categorical columns\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical pipelines\n",
    "preprocessing_pipeline = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_columns),\n",
    "    ('categorical', categorical_pipeline, categorical_columns)\n",
    "])\n",
    "\n",
    "# Final pipeline with feature selection and model\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessing', preprocessing_pipeline),  # Preprocessing steps\n",
    "    ('feature_selection', feature_selection),  # Feature selection\n",
    "    ('regressor', RandomForestRegressor())  # Random Forest Regressor\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the model on the test dataset\n",
    "accuracy = final_pipeline.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03321a2e-d8b2-436c-927d-64d9b75fddb4",
   "metadata": {},
   "source": [
    "## Q2. Build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then use a Voting Classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7def4ad-ab3c-48d6-8fdc-c1dc4fefbb5d",
   "metadata": {},
   "source": [
    "Below is the code to build a pipeline that includes a Random Forest Classifier and a Logistic Regression Classifier, and then uses a Voting Classifier to combine their predictions on the iris dataset and evaluates its accuracy:\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "In this code:\n",
    "- We load the iris dataset and split it into training and testing sets.\n",
    "- We define a Random Forest Classifier and a Logistic Regression Classifier.\n",
    "- We create a pipeline that scales the features and then uses a Voting Classifier to combine the predictions of the two classifiers using soft voting.\n",
    "- We train the pipeline on the training dataset.\n",
    "- We evaluate the accuracy of the pipeline on the test dataset using accuracy_score from sklearn.metrics.\n",
    "\n",
    "This pipeline combines the predictions of a Random Forest Classifier and a Logistic Regression Classifier using soft voting, which can often improve overall predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f00f94da-7621-4258-ac19-f9f36c8f0d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the classifiers\n",
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "lr_classifier = LogisticRegression(random_state=42)\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Scale features\n",
    "    ('voting', VotingClassifier(estimators=[\n",
    "        ('rf', rf_classifier),\n",
    "        ('lr', lr_classifier)\n",
    "    ], voting='soft'))  # Combine predictions using soft voting\n",
    "])\n",
    "\n",
    "# Train the pipeline on the training dataset\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the accuracy of the pipeline on the test dataset\n",
    "y_pred = pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6205445-5820-44bc-a7a4-f18252867def",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
