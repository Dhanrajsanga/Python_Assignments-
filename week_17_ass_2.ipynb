{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75adab90-879c-460f-b733-26d7d9d7608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "# company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "# probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf803b48-d044-4af3-afae-51188a7c228f",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that he/she uses the health insurance plan, we can use Bayes' theorem. Let's denote the events as follows:\n",
    "\n",
    "- Event \\( A \\): Employee uses the health insurance plan.\n",
    "- Event \\( B \\): Employee is a smoker.\n",
    "\n",
    "We are given:\n",
    "\n",
    "- \\( P(A) \\), the probability that an employee uses the health insurance plan, which is 70% or 0.70.\n",
    "- \\( P(B|A) \\), the conditional probability that an employee is a smoker given that they use the health insurance plan, which is 40% or 0.40.\n",
    "\n",
    "We need to find \\( P(B|A) \\), the probability that an employee is a smoker given that they use the health insurance plan. We can use Bayes' theorem to calculate this:\n",
    "\n",
    "\\[ P(B|A) = \\frac{P(A|B) \\times P(B)}{P(A)} \\]\n",
    "\n",
    "Given that \\( P(A) = 0.70 \\) and \\( P(B|A) = 0.40 \\), we can calculate \\( P(B) \\), the probability that an employee is a smoker:\n",
    "\n",
    "\\[ P(B) = \\frac{P(A|B) \\times P(B)}{P(A)} \\]\n",
    "\n",
    "\\[ P(B) = \\frac{0.40 \\times 0.70}{0.70} \\]\n",
    "\n",
    "\\[ P(B) = 0.40 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that he/she uses the health insurance plan is 40%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b630ec1-6383-47de-8342-78b7ad7254f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30facfd2-a609-4a90-8ab3-692068f4dfb5",
   "metadata": {},
   "source": [
    "The main difference between Bernoulli Naive Bayes and Multinomial Naive Bayes lies in the type of data they are designed to handle and the underlying assumptions about the distribution of features:\n",
    "\n",
    "1. **Bernoulli Naive Bayes**:\n",
    "   - Bernoulli Naive Bayes is suitable for binary feature data, where features represent presence or absence of certain attributes.\n",
    "   - It assumes that features are independent binary variables, meaning each feature is considered as a binary variable that follows a Bernoulli distribution.\n",
    "   - It is commonly used in text classification tasks, such as document classification or sentiment analysis, where features represent the presence or absence of words in documents.\n",
    "   - Example: Spam email detection, where features represent the presence or absence of specific keywords in an email.\n",
    "\n",
    "2. **Multinomial Naive Bayes**:\n",
    "   - Multinomial Naive Bayes is suitable for data with features that represent counts or frequencies, such as word counts in text data.\n",
    "   - It assumes that features are independent variables following a multinomial distribution, where each feature represents the frequency of occurrence of a particular attribute.\n",
    "   - It is commonly used in text classification tasks, similar to Bernoulli Naive Bayes, but it considers the frequency of words rather than just their presence or absence.\n",
    "   - Example: Document classification based on word counts, where features represent the frequency of words in documents.\n",
    "\n",
    "In summary, Bernoulli Naive Bayes is used for binary feature data, while Multinomial Naive Bayes is used for data with features representing counts or frequencies. The choice between the two depends on the nature of the data and the assumptions about the distribution of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67f9947d-d6d1-4346-a590-1a5beaab5ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd7fd01-b063-4870-90bf-be83a62eaba6",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes typically does not handle missing values explicitly. In most implementations, missing values are either ignored or replaced with a default value before training the model. \n",
    "\n",
    "Here are common approaches to handling missing values in Bernoulli Naive Bayes:\n",
    "\n",
    "1. **Ignore Missing Values**: Some implementations of Bernoulli Naive Bayes simply ignore instances with missing values during model training and prediction. This means that any instance containing missing values is excluded from the analysis.\n",
    "\n",
    "2. **Imputation**: Another approach is to impute missing values with a default value, such as 0 or 1, depending on the context of the data. For example, in a binary feature dataset, missing values might be imputed with the mode (most common value) of the feature.\n",
    "\n",
    "3. **Data Preprocessing**: Before applying Bernoulli Naive Bayes, you can preprocess the data to handle missing values using techniques such as mean imputation, median imputation, or interpolation. Once the missing values are imputed, the data can be used to train the Bernoulli Naive Bayes model.\n",
    "\n",
    "It's important to note that the choice of handling missing values depends on the specific characteristics of the dataset and the assumptions about the nature of the missingness. It's advisable to carefully consider the implications of each approach and experiment with different strategies to determine the most suitable method for your particular dataset and problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4363832b-3bc0-4bad-9303-4e359640da90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2fc6f9-27f6-417b-9370-99e6f5d10f6a",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification tasks. While it is often used for binary classification problems, Gaussian Naive Bayes can be extended to handle multi-class classification by using the \"one-vs-all\" (also known as \"one-vs-rest\") strategy.\n",
    "\n",
    "In the one-vs-all strategy, a separate binary classifier is trained for each class, where the data for one class is labeled as positive and the data for all other classes are labeled as negative. During prediction, the class with the highest probability output from the binary classifiers is assigned as the predicted class for the input instance.\n",
    "\n",
    "Here's how Gaussian Naive Bayes can be adapted for multi-class classification:\n",
    "\n",
    "1. **Training**:\n",
    "   - For each class \\(i\\) in the multi-class problem:\n",
    "     - Assign the data points belonging to class \\(i\\) as positive examples and the data points belonging to all other classes as negative examples.\n",
    "     - Train a separate Gaussian Naive Bayes classifier for class \\(i\\) using the labeled data.\n",
    "\n",
    "2. **Prediction**:\n",
    "   - For a new input instance, obtain the probability estimates for each class from all the trained binary classifiers.\n",
    "   - Assign the class with the highest probability as the predicted class for the input instance.\n",
    "\n",
    "By using the one-vs-all strategy, Gaussian Naive Bayes can effectively handle multi-class classification problems. However, it's important to note that the performance of the classifier may vary depending on the nature of the data and the assumptions made by the Gaussian Naive Bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05cf94d4-02e0-4eed-9410-3e9c4db8a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# Data preparation:\n",
    "# Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "# datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "# is spam or not based on several input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56d2a567-ea30-4d81-97a9-0bedd99083fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 94, 'name': 'Spambase', 'repository_url': 'https://archive.ics.uci.edu/dataset/94/spambase', 'data_url': 'https://archive.ics.uci.edu/static/public/94/data.csv', 'abstract': 'Classifying Email as Spam or Non-Spam', 'area': 'Computer Science', 'tasks': ['Classification'], 'characteristics': ['Multivariate'], 'num_instances': 4601, 'num_features': 57, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': ['Class'], 'index_col': None, 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 1999, 'last_updated': 'Mon Aug 28 2023', 'dataset_doi': '10.24432/C53G6X', 'creators': ['Mark Hopkins', 'Erik Reeber', 'George Forman', 'Jaap Suermondt'], 'intro_paper': None, 'additional_info': {'summary': 'The \"spam\" concept is diverse: advertisements for products/web sites, make money fast schemes, chain letters, pornography...\\n\\nThe classification task for this dataset is to determine whether a given email is spam or not.\\n\\t\\nOur collection of spam e-mails came from our postmaster and individuals who had filed spam.  Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word \\'george\\' and the area code \\'650\\' are indicators of non-spam.  These are useful when constructing a personalized spam filter.  One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter.\\n\\nFor background on spam: Cranor, Lorrie F., LaMacchia, Brian A.  Spam!, Communications of the ACM, 41(8):74-83, 1998.\\n\\nTypical performance is around ~7% misclassification error. False positives (marking good mail as spam) are very undesirable.If we insist on zero false positives in the training/testing set, 20-25% of the spam passed through the filter. See also Hewlett-Packard Internal-only Technical Report. External version forthcoming. ', 'purpose': None, 'funded_by': None, 'instances_represent': 'Emails', 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': 'The last column of \\'spambase.data\\' denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.  The run-length attributes (55-57) measure the length of sequences of consecutive capital letters.  For the statistical measures of each attribute, see the end of this file.  Here are the definitions of the attributes:\\r\\n\\r\\n48 continuous real [0,100] attributes of type word_freq_WORD \\r\\n= percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A \"word\" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.\\r\\n\\r\\n6 continuous real [0,100] attributes of type char_freq_CHAR] \\r\\n= percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail\\r\\n\\r\\n1 continuous real [1,...] attribute of type capital_run_length_average \\r\\n= average length of uninterrupted sequences of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_longest \\r\\n= length of longest uninterrupted sequence of capital letters\\r\\n\\r\\n1 continuous integer [1,...] attribute of type capital_run_length_total \\r\\n= sum of length of uninterrupted sequences of capital letters \\r\\n= total number of capital letters in the e-mail\\r\\n\\r\\n1 nominal {0,1} class attribute of type spam\\r\\n= denotes whether the e-mail was considered spam (1) or not (0), i.e. unsolicited commercial e-mail.  \\r\\n', 'citation': None}}\n",
      "                          name     role        type demographic  \\\n",
      "0               word_freq_make  Feature  Continuous        None   \n",
      "1            word_freq_address  Feature  Continuous        None   \n",
      "2                word_freq_all  Feature  Continuous        None   \n",
      "3                 word_freq_3d  Feature  Continuous        None   \n",
      "4                word_freq_our  Feature  Continuous        None   \n",
      "5               word_freq_over  Feature  Continuous        None   \n",
      "6             word_freq_remove  Feature  Continuous        None   \n",
      "7           word_freq_internet  Feature  Continuous        None   \n",
      "8              word_freq_order  Feature  Continuous        None   \n",
      "9               word_freq_mail  Feature  Continuous        None   \n",
      "10           word_freq_receive  Feature  Continuous        None   \n",
      "11              word_freq_will  Feature  Continuous        None   \n",
      "12            word_freq_people  Feature  Continuous        None   \n",
      "13            word_freq_report  Feature  Continuous        None   \n",
      "14         word_freq_addresses  Feature  Continuous        None   \n",
      "15              word_freq_free  Feature  Continuous        None   \n",
      "16          word_freq_business  Feature  Continuous        None   \n",
      "17             word_freq_email  Feature  Continuous        None   \n",
      "18               word_freq_you  Feature  Continuous        None   \n",
      "19            word_freq_credit  Feature  Continuous        None   \n",
      "20              word_freq_your  Feature  Continuous        None   \n",
      "21              word_freq_font  Feature  Continuous        None   \n",
      "22               word_freq_000  Feature  Continuous        None   \n",
      "23             word_freq_money  Feature  Continuous        None   \n",
      "24                word_freq_hp  Feature  Continuous        None   \n",
      "25               word_freq_hpl  Feature  Continuous        None   \n",
      "26            word_freq_george  Feature  Continuous        None   \n",
      "27               word_freq_650  Feature  Continuous        None   \n",
      "28               word_freq_lab  Feature  Continuous        None   \n",
      "29              word_freq_labs  Feature  Continuous        None   \n",
      "30            word_freq_telnet  Feature  Continuous        None   \n",
      "31               word_freq_857  Feature  Continuous        None   \n",
      "32              word_freq_data  Feature  Continuous        None   \n",
      "33               word_freq_415  Feature  Continuous        None   \n",
      "34                word_freq_85  Feature  Continuous        None   \n",
      "35        word_freq_technology  Feature  Continuous        None   \n",
      "36              word_freq_1999  Feature  Continuous        None   \n",
      "37             word_freq_parts  Feature  Continuous        None   \n",
      "38                word_freq_pm  Feature  Continuous        None   \n",
      "39            word_freq_direct  Feature  Continuous        None   \n",
      "40                word_freq_cs  Feature  Continuous        None   \n",
      "41           word_freq_meeting  Feature  Continuous        None   \n",
      "42          word_freq_original  Feature  Continuous        None   \n",
      "43           word_freq_project  Feature  Continuous        None   \n",
      "44                word_freq_re  Feature  Continuous        None   \n",
      "45               word_freq_edu  Feature  Continuous        None   \n",
      "46             word_freq_table  Feature  Continuous        None   \n",
      "47        word_freq_conference  Feature  Continuous        None   \n",
      "48                 char_freq_;  Feature  Continuous        None   \n",
      "49                 char_freq_(  Feature  Continuous        None   \n",
      "50                 char_freq_[  Feature  Continuous        None   \n",
      "51                 char_freq_!  Feature  Continuous        None   \n",
      "52                 char_freq_$  Feature  Continuous        None   \n",
      "53                 char_freq_#  Feature  Continuous        None   \n",
      "54  capital_run_length_average  Feature  Continuous        None   \n",
      "55  capital_run_length_longest  Feature  Continuous        None   \n",
      "56    capital_run_length_total  Feature  Continuous        None   \n",
      "57                       Class   Target      Binary        None   \n",
      "\n",
      "                 description units missing_values  \n",
      "0                       None  None             no  \n",
      "1                       None  None             no  \n",
      "2                       None  None             no  \n",
      "3                       None  None             no  \n",
      "4                       None  None             no  \n",
      "5                       None  None             no  \n",
      "6                       None  None             no  \n",
      "7                       None  None             no  \n",
      "8                       None  None             no  \n",
      "9                       None  None             no  \n",
      "10                      None  None             no  \n",
      "11                      None  None             no  \n",
      "12                      None  None             no  \n",
      "13                      None  None             no  \n",
      "14                      None  None             no  \n",
      "15                      None  None             no  \n",
      "16                      None  None             no  \n",
      "17                      None  None             no  \n",
      "18                      None  None             no  \n",
      "19                      None  None             no  \n",
      "20                      None  None             no  \n",
      "21                      None  None             no  \n",
      "22                      None  None             no  \n",
      "23                      None  None             no  \n",
      "24                      None  None             no  \n",
      "25                      None  None             no  \n",
      "26                      None  None             no  \n",
      "27                      None  None             no  \n",
      "28                      None  None             no  \n",
      "29                      None  None             no  \n",
      "30                      None  None             no  \n",
      "31                      None  None             no  \n",
      "32                      None  None             no  \n",
      "33                      None  None             no  \n",
      "34                      None  None             no  \n",
      "35                      None  None             no  \n",
      "36                      None  None             no  \n",
      "37                      None  None             no  \n",
      "38                      None  None             no  \n",
      "39                      None  None             no  \n",
      "40                      None  None             no  \n",
      "41                      None  None             no  \n",
      "42                      None  None             no  \n",
      "43                      None  None             no  \n",
      "44                      None  None             no  \n",
      "45                      None  None             no  \n",
      "46                      None  None             no  \n",
      "47                      None  None             no  \n",
      "48                      None  None             no  \n",
      "49                      None  None             no  \n",
      "50                      None  None             no  \n",
      "51                      None  None             no  \n",
      "52                      None  None             no  \n",
      "53                      None  None             no  \n",
      "54                      None  None             no  \n",
      "55                      None  None             no  \n",
      "56                      None  None             no  \n",
      "57  spam (1) or not spam (0)  None             no  \n"
     ]
    }
   ],
   "source": [
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "spambase = fetch_ucirepo(id=94) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = spambase.data.features \n",
    "y = spambase.data.targets \n",
    "  \n",
    "# metadata \n",
    "print(spambase.metadata) \n",
    "  \n",
    "# variable information \n",
    "print(spambase.variables) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2495d19-9837-4a75-8492-95eff0de491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation:\n",
    "# Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "# scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "# dataset. You should use the default hyperparameters for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7e08483-3aaf-4afd-9ece-2eb23338494d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Mean Accuracy: 0.8839380364047911\n",
      "Multinomial Naive Bayes Mean Accuracy: 0.7863496180326323\n",
      "Gaussian Naive Bayes Mean Accuracy: 0.8217730830896915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "\n",
    "# Load the dataset (replace 'dataset_name' with the actual name of the dataset)\n",
    "dataset = fetch_openml(name='spambase', version=1)\n",
    "\n",
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "# Initialize classifiers\n",
    "bernoulli_nb = BernoulliNB()\n",
    "multinomial_nb = MultinomialNB()\n",
    "gaussian_nb = GaussianNB()\n",
    "\n",
    "# Perform 10-fold cross-validation for each classifier\n",
    "# Bernoulli Naive Bayes\n",
    "bernoulli_scores = cross_val_score(bernoulli_nb, X, y, cv=10)\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "multinomial_scores = cross_val_score(multinomial_nb, X, y, cv=10)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gaussian_scores = cross_val_score(gaussian_nb, X, y, cv=10)\n",
    "\n",
    "# Print the mean accuracy of each classifier\n",
    "print(\"Bernoulli Naive Bayes Mean Accuracy:\", bernoulli_scores.mean())\n",
    "print(\"Multinomial Naive Bayes Mean Accuracy:\", multinomial_scores.mean())\n",
    "print(\"Gaussian Naive Bayes Mean Accuracy:\", gaussian_scores.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4b06f-cd39-43ac-bf2d-1d70b392482e",
   "metadata": {},
   "source": [
    "In this implementation:\n",
    "\n",
    "We load the dataset using fetch_openml() function from scikit-learn, assuming the dataset is available in the OpenML repository.\n",
    "We split the dataset into features (X) and target variable (y).\n",
    "We initialize Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the respective classes from scikit-learn.\n",
    "We perform 10-fold cross-validation for each classifier using the cross_val_score() function.\n",
    "Finally, we print the mean accuracy of each classifier across all folds of the cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ea01fa4-e089-4c5c-ae84-c28deda7dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results:\n",
    "# Report the following performance metrics for each classifier:\n",
    "# Accuracy\n",
    "# Precision\n",
    "# Recall\n",
    "# F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512a7d7-1022-464a-b0bb-4b44512addbf",
   "metadata": {},
   "source": [
    "To report the performance metrics for each classifier (Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes), we can calculate the following metrics using the cross-validation results:\n",
    "\n",
    "1. Accuracy: The proportion of correctly classified instances.\n",
    "2. Precision: The ratio of true positive predictions to the total number of positive predictions.\n",
    "3. Recall: The ratio of true positive predictions to the total number of actual positive instances.\n",
    "4. F1 score: The harmonic mean of precision and recall, providing a balanced measure between the two.\n",
    "\n",
    "Here's how we can calculate these metrics using the cross-validation scores:\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "This code calculates and prints the accuracy, precision, recall, and F1 score for each classifier based on the cross-validation results. Make sure to replace `y_true` with the true target values and `y_pred` with the predicted values for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f239f9aa-85d0-44b4-8ccf-91774e9724f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Calculate performance metrics for Bernoulli Naive Bayes\n",
    "bernoulli_accuracy = accuracy_score(y_true=y, y_pred=bernoulli_scores)\n",
    "bernoulli_precision = precision_score(y_true=y, y_pred=bernoulli_scores)\n",
    "bernoulli_recall = recall_score(y_true=y, y_pred=bernoulli_scores)\n",
    "bernoulli_f1 = f1_score(y_true=y, y_pred=bernoulli_scores)\n",
    "\n",
    "# Calculate performance metrics for Multinomial Naive Bayes\n",
    "multinomial_accuracy = accuracy_score(y_true=y, y_pred=multinomial_scores)\n",
    "multinomial_precision = precision_score(y_true=y, y_pred=multinomial_scores)\n",
    "multinomial_recall = recall_score(y_true=y, y_pred=multinomial_scores)\n",
    "multinomial_f1 = f1_score(y_true=y, y_pred=multinomial_scores)\n",
    "\n",
    "# Calculate performance metrics for Gaussian Naive Bayes\n",
    "gaussian_accuracy = accuracy_score(y_true=y, y_pred=gaussian_scores)\n",
    "gaussian_precision = precision_score(y_true=y, y_pred=gaussian_scores)\n",
    "gaussian_recall = recall_score(y_true=y, y_pred=gaussian_scores)\n",
    "gaussian_f1 = f1_score(y_true=y, y_pred=gaussian_scores)\n",
    "\n",
    "# Print the performance metrics for each classifier\n",
    "print(\"Results:\")\n",
    "print(\"Performance Metrics for Bernoulli Naive Bayes:\")\n",
    "print(\"Accuracy:\", bernoulli_accuracy)\n",
    "print(\"Precision:\", bernoulli_precision)\n",
    "print(\"Recall:\", bernoulli_recall)\n",
    "print(\"F1 Score:\", bernoulli_f1)\n",
    "print()\n",
    "\n",
    "print(\"Performance Metrics for Multinomial Naive Bayes:\")\n",
    "print(\"Accuracy:\", multinomial_accuracy)\n",
    "print(\"Precision:\", multinomial_precision)\n",
    "print(\"Recall:\", multinomial_recall)\n",
    "print(\"F1 Score:\", multinomial_f1)\n",
    "print()\n",
    "\n",
    "print(\"Performance Metrics for Gaussian Naive Bayes:\")\n",
    "print(\"Accuracy:\", gaussian_accuracy)\n",
    "print(\"Precision:\", gaussian_precision)\n",
    "print(\"Recall:\", gaussian_recall)\n",
    "print(\"F1 Score:\", gaussian_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b7da514-5f96-435c-a489-dc63e7e4393b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discussion:\n",
    "# Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "# the case? Are there any limitations of Naive Bayes that you observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb9c20-4945-42a3-8b44-59b6331b7988",
   "metadata": {},
   "source": [
    "Based on the results obtained from evaluating the performance metrics for each variant of Naive Bayes (Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes), we can discuss the following observations:\n",
    "\n",
    "1. **Performance Comparison**:\n",
    "   - The performance of each Naive Bayes variant can be assessed based on metrics such as accuracy, precision, recall, and F1 score.\n",
    "   - After evaluating these metrics, we can compare the performance of each variant to determine which one performed the best overall.\n",
    "\n",
    "2. **Best Performing Variant**:\n",
    "   - The variant of Naive Bayes that performs the best is typically the one with the highest values for the performance metrics.\n",
    "   - It's essential to consider all metrics collectively rather than relying solely on one metric to determine the best-performing variant.\n",
    "\n",
    "3. **Reasons for Performance**:\n",
    "   - The reason one variant of Naive Bayes may outperform others could be due to the nature of the dataset and how well the assumptions of each variant align with the underlying data distribution.\n",
    "   - For instance, if the dataset consists of binary features, Bernoulli Naive Bayes might perform better as it assumes features follow a Bernoulli distribution.\n",
    "   - Similarly, if the dataset contains features representing counts or frequencies, Multinomial Naive Bayes might be more suitable as it assumes features follow a multinomial distribution.\n",
    "\n",
    "4. **Limitations of Naive Bayes**:\n",
    "   - Despite its simplicity and efficiency, Naive Bayes classifiers make strong independence assumptions between features, which might not hold true in all datasets.\n",
    "   - Another limitation is the sensitivity to the presence of irrelevant or correlated features, which can adversely affect performance.\n",
    "   - Moreover, Naive Bayes classifiers may struggle with datasets that have imbalanced class distributions, as they assume equal prior probabilities for each class.\n",
    "\n",
    "Overall, while Naive Bayes classifiers are often considered robust and efficient, their performance can vary depending on the specific characteristics of the dataset and how well their assumptions align with the underlying data distribution. It's essential to carefully evaluate the performance of each variant and consider their limitations when applying Naive Bayes in real-world scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eaca690-12c6-42fb-88ed-1ccdb4aba359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conclusion:\n",
    "# Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da69b12-0577-4b1f-9e46-80a567dd6cc2",
   "metadata": {},
   "source": [
    "In conclusion, our evaluation of different variants of Naive Bayes classifiers (Bernoulli, Multinomial, and Gaussian) yielded insights into their performance on a binary classification task. Here are the key findings and suggestions for future work:\n",
    "\n",
    "1. **Findings**:\n",
    "   - Each variant of Naive Bayes exhibited varying performance across different evaluation metrics such as accuracy, precision, recall, and F1 score.\n",
    "   - The best-performing variant was determined based on the overall performance across all metrics, considering the specific characteristics of the dataset.\n",
    "   - The choice of the best-performing variant may depend on the nature of the dataset and how well the assumptions of each variant align with the underlying data distribution.\n",
    "\n",
    "2. **Suggestions for Future Work**:\n",
    "   - Investigate Ensemble Methods: Future work could explore ensemble methods, such as Bagging or Boosting, to combine predictions from multiple Naive Bayes classifiers and potentially improve classification performance.\n",
    "   - Feature Engineering: Conduct further feature engineering to identify and select relevant features or to transform existing features to better align with the assumptions of each Naive Bayes variant.\n",
    "   - Address Class Imbalance: Implement techniques to address class imbalance, such as oversampling, undersampling, or using different evaluation metrics tailored for imbalanced datasets, to enhance the performance of Naive Bayes classifiers.\n",
    "   - Evaluate on Diverse Datasets: Test the performance of Naive Bayes classifiers on a diverse range of datasets with varying characteristics to gain a deeper understanding of their strengths and limitations across different domains.\n",
    "\n",
    "By addressing these suggestions and conducting further research, we can gain a better understanding of the applicability and performance of Naive Bayes classifiers in practical scenarios and potentially enhance their effectiveness in real-world applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d9bb90-cf7b-441a-a0c8-aeafd6134824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
