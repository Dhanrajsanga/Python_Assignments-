{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ade4501-2295-4800-979d-34e743f4c7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "# algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d98498-71cb-4564-803a-f7012fc1b930",
   "metadata": {},
   "source": [
    "In machine learning algorithms, kernel functions are often used in the context of support vector machines (SVMs) to transform data into a higher-dimensional space where it can be linearly separated. Polynomial functions are a type of kernel function commonly used in SVMs. The relationship between polynomial functions and kernel functions lies in the fact that polynomial kernel functions compute the dot product between data points in the transformed space, allowing SVMs to effectively learn non-linear decision boundaries. Polynomial kernel functions are particularly useful when the decision boundary between classes is non-linear and can be represented as a polynomial curve or surface in the original feature space. Thus, polynomial functions serve as the basis for constructing polynomial kernel functions, enabling SVMs to handle non-linear classification tasks efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e28b2407-71d9-487b-be5d-fda4a9c8218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d22f437-e8c6-4ccf-adc7-2c577a95001f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate some example data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize an SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)  # You can adjust the degree of the polynomial\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3caed2-481c-4d2d-9938-47031e5f157c",
   "metadata": {},
   "source": [
    "In this example:\n",
    "\n",
    "We first generate some example data using the make_classification function from Scikit-learn.\n",
    "Then, we split the data into training and testing sets using the train_test_split function.\n",
    "Next, we initialize an SVM classifier using the SVC class from Scikit-learn and specify the polynomial kernel by setting kernel='poly'. Additionally, you can adjust the degree of the polynomial using the degree parameter.\n",
    "We train the SVM classifier on the training data using the fit method.\n",
    "After training, we make predictions on the test data using the predict method.\n",
    "Finally, we calculate the accuracy of the classifier on the test data using the accuracy_score function from Scikit-learn.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbe709f-d099-41ed-a77f-2aa45ab2c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9821c9-bbfe-4e18-a90f-c4a9ae9fe621",
   "metadata": {},
   "source": [
    "In Support Vector Regression (SVR), epsilon (ε) is a hyperparameter that controls the width of the margin around the predicted function within which no penalty is incurred. This margin is called the ε-insensitive tube. Increasing the value of epsilon allows for a larger margin, meaning that deviations of the predicted values from the actual target values within this margin are ignored and do not contribute to the loss function.\n",
    "\n",
    "When you increase the value of epsilon in SVR, you effectively widen the ε-insensitive tube. As a result, more data points can fall within this wider margin without incurring a penalty. Consequently, the SVR model becomes less sensitive to small fluctuations in the training data that lie within this margin.\n",
    "\n",
    "The number of support vectors in SVR is influenced by the width of this margin. When epsilon is increased, the margin widens, and more data points may fall within it, potentially reducing the number of support vectors. This is because support vectors are the data points lying on the boundary of the margin or within the margin, and when the margin widens, fewer data points are needed to define it.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR typically leads to a reduction in the number of support vectors, as it allows for a wider margin around the predicted function, accommodating more data points without penalty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b42e2cd8-c636-49d9-9383-9326189a3a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "# affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "# and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18842af4-b1c2-4372-867f-4108ccce7d5c",
   "metadata": {},
   "source": [
    "The performance of Support Vector Regression (SVR) is highly influenced by several key parameters: the choice of kernel function, the C parameter, the epsilon parameter (ε), and the gamma parameter (γ). Let's discuss each parameter and its effects:\n",
    "\n",
    "1. **Kernel Function**:\n",
    "   - The kernel function determines the type of transformation applied to the input data. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid.\n",
    "   - Example: If the data is linearly separable, using a linear kernel might be sufficient. However, if the data is non-linear, using a polynomial or RBF kernel might provide better results.\n",
    "\n",
    "2. **C Parameter**:\n",
    "   - The C parameter controls the trade-off between maximizing the margin and minimizing the training error. A smaller C value encourages a wider margin but may lead to more training errors, while a larger C value allows for a narrower margin but may result in fewer training errors.\n",
    "   - Example: If the training data contains outliers or noise, it may be beneficial to decrease the value of C to allow for a wider margin and reduce the influence of individual data points. Conversely, if the training data is clean and well-behaved, increasing C may lead to better generalization.\n",
    "\n",
    "3. **Epsilon Parameter (ε)**:\n",
    "   - The epsilon parameter determines the size of the ε-insensitive tube, which defines the range within which errors are not penalized. Data points lying within this tube do not contribute to the loss function.\n",
    "   - Example: Increasing ε allows for a larger margin of tolerance for errors, which can help in capturing a smoother regression function. However, setting ε too large may lead to underfitting, while setting it too small may result in overfitting.\n",
    "\n",
    "4. **Gamma Parameter (γ)**:\n",
    "   - The gamma parameter defines the influence of each training example on the decision boundary. A smaller gamma value makes the decision boundary smoother, while a larger gamma value makes the boundary more complex, potentially leading to overfitting.\n",
    "   - Example: In the case of the RBF kernel, increasing γ can lead to a tighter fit to the training data, which may be desirable when the data is densely packed or exhibits complex patterns. However, increasing γ too much can lead to overfitting and poor generalization.\n",
    "\n",
    "In summary, the choice of kernel function, along with the tuning of the C, ε, and γ parameters, significantly affects the performance of SVR. Understanding the characteristics of the data and the impact of each parameter is crucial for selecting appropriate values that balance model complexity, generalization ability, and accuracy. Cross-validation techniques can be used to systematically search for the optimal combination of parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812d02df-4534-4096-9e11-e9c8380f2d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Assignment:\n",
    "# L Import the necessary libraries and load the dataseg\n",
    "# L Split the dataset into training and testing setZ\n",
    "# L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "# L Create an instance of the SVC classifier and train it on the training datW\n",
    "# L hse the trained classifier to predict the labels of the testing datW\n",
    "# L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "# precision, recall, F1-scoreK\n",
    "# L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "# improve its performanc_\n",
    "# L Train the tuned classifier on the entire dataseg\n",
    "# L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ce1f5c-6e27-4da6-a896-9b0f21279268",
   "metadata": {},
   "source": [
    "Sure, let's go step by step through the assignment:\n",
    "\n",
    "This example uses the Iris dataset, standard scaling for preprocessing, and evaluates the classifier's accuracy. You can modify the dataset, preprocessing techniques, and evaluation metrics based on your specific requirements. Additionally, you can explore other hyperparameter tuning methods like RandomizedSearchCV for further optimization. The tuned classifier is saved to a file using joblib for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72b03703-bb76-4f5a-8ecc-ede860bb910d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Best Hyperparameters: {'C': 0.1, 'gamma': 'scale', 'kernel': 'linear'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tuned_svc_classifier.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "\n",
    "# Load the dataset (Example: Using the Iris dataset)\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the data (Standard Scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier\n",
    "svc_classifier = SVC()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "svc_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Use the trained classifier to predict labels on the testing data\n",
    "y_pred = svc_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the performance using accuracy as the metric\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf', 'poly'], 'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=3)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "tuned_svc_classifier = SVC(**best_params)\n",
    "tuned_svc_classifier.fit(X_train_scaled, y_train)  # Use X_train_scaled here\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(tuned_svc_classifier, 'tuned_svc_classifier.joblib')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2301cf4f-19e2-40d8-a13f-57f2689d9b01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
