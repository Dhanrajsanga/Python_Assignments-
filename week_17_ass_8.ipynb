{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37386688-7edf-4ad1-9a62-8b151e25a7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246d6efa-7869-4ca1-a505-01a9577e65c6",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a powerful machine learning technique used for regression tasks. It is an ensemble learning method that builds a strong predictive model by combining the predictions of multiple individual regression models, typically decision trees. Here's an overview of Gradient Boosting Regression:\n",
    "\n",
    "1. **Sequential Training**: Gradient Boosting Regression trains a series of decision trees sequentially, with each tree learning to correct the errors (residuals) made by the previous ones.\n",
    "\n",
    "2. **Gradient Descent**: Unlike traditional gradient descent optimization methods that update model parameters directly, Gradient Boosting Regression optimizes the model by minimizing a loss function with respect to the residuals. It uses gradient descent to find the optimal direction to update the model's parameters (e.g., tree structure, leaf values) at each iteration.\n",
    "\n",
    "3. **Weak Learners (Decision Trees)**: The weak learners in Gradient Boosting Regression are typically shallow decision trees, also known as regression trees. These trees are simple and have limited depth to prevent overfitting.\n",
    "\n",
    "4. **Boosting Mechanism**: Gradient Boosting Regression employs a boosting mechanism to combine the predictions of multiple weak learners into a strong ensemble model. Each subsequent tree is trained to predict the residuals (the difference between the actual and predicted values) of the previous trees, gradually reducing the overall error of the ensemble.\n",
    "\n",
    "5. **Regularization**: Gradient Boosting Regression often incorporates regularization techniques, such as tree pruning, learning rate adjustment, and early stopping, to prevent overfitting and improve the generalization performance of the model.\n",
    "\n",
    "6. **Hyperparameter Tuning**: Gradient Boosting Regression requires tuning several hyperparameters, such as the number of trees (iterations), tree depth, learning rate, and regularization parameters, to optimize the model's performance on the validation data.\n",
    "\n",
    "7. **Prediction**: To make predictions, Gradient Boosting Regression aggregates the predictions of all weak learners in the ensemble. The final prediction is the sum of the predictions of all trees, adjusted by the learning rate.\n",
    "\n",
    "Overall, Gradient Boosting Regression is a highly effective and widely used regression technique known for its ability to produce accurate predictions and handle complex nonlinear relationships in the data. It is commonly used in various domains, including finance, healthcare, and marketing, where accurate regression modeling is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8400720e-e1ec-4749-8108-cfa73bed898f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "# simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "# performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9daa3609-c034-4630-b074-b5d1a6d9eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 115637.64530316826\n",
      "R-squared: -81.94039755800658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292/2282886246.py:31: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n",
      "  predictions = np.sum(tree.predict(X) for tree in self.trees)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with mean of target variable\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        # Train ensemble of regression trees\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train regression tree on residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Add trained tree to ensemble\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions by summing predictions of all trees\n",
    "        predictions = np.sum(tree.predict(X) for tree in self.trees)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on testing set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48523b16-1d14-4cbd-b03b-92ffbe6bf159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "# optimise the performance of the model. Use grid search or random search to find the best\n",
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0ba46ab-afe1-4d5f-925e-1c3ab9da4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 31.735482349161565\n",
      "R-squared: 0.9772379183627112\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import chain\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initialize predictions with mean of target variable\n",
    "        predictions = np.full_like(y, np.mean(y))\n",
    "\n",
    "        # Train ensemble of regression trees\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train regression tree on residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "            # Add trained tree to ensemble\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "    # Make predictions by summing predictions of all trees\n",
    "        predictions = np.zeros(len(X))\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X).flatten()\n",
    "        return predictions\n",
    "\n",
    "\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize gradient boosting regressor\n",
    "gb_regressor = GradientBoostingRegressor()\n",
    "\n",
    "# Fit model\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on testing set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb0f0c72-3d32-42e4-bbdd-212c7ff023c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050abd09-c0fc-40ee-ba69-866fc69bbe0e",
   "metadata": {},
   "source": [
    "In Gradient Boosting, a weak learner refers to a base model, typically a decision tree with limited depth or complexity, that performs slightly better than random guessing on a given problem. The term \"weak\" does not imply that the model is inherently poor, but rather that it is not powerful enough to make accurate predictions on its own. \n",
    "\n",
    "In the context of Gradient Boosting, the weak learner's predictions are combined with those of other weak learners in an iterative manner to gradually improve the overall predictive performance of the ensemble. Each weak learner focuses on capturing different aspects of the data, and their collective strength lies in their ability to complement each other and collectively form a strong predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "367edb7e-ded8-4698-9180-48c19e95cb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f94362-e368-4295-aaf2-9fca996d2a50",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to sequentially build an ensemble of weak learners, typically decision trees, where each subsequent learner corrects the errors made by the previous ones. \n",
    "\n",
    "Here's a simplified explanation of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. Start with an initial weak learner, such as a decision tree, which makes predictions based on the input features.\n",
    "\n",
    "2. Train the weak learner on the data and calculate the errors or residuals, which represent the differences between the actual and predicted values.\n",
    "\n",
    "3. Build a new weak learner to predict these residuals. This second learner aims to capture the patterns or relationships in the data that were not captured by the first learner.\n",
    "\n",
    "4. Combine the predictions of the two learners by adding them together. This combined prediction is closer to the actual target values than the prediction of the first learner alone.\n",
    "\n",
    "5. Repeat the process by training additional weak learners, each one focusing on reducing the errors made by the ensemble of learners constructed so far.\n",
    "\n",
    "6. Finally, combine the predictions of all the weak learners to produce the final ensemble prediction, which is typically a weighted sum of the predictions of each individual weak learner.\n",
    "\n",
    "By iteratively fitting new weak learners to the residuals of the previous ensemble and combining their predictions, Gradient Boosting gradually improves the overall predictive performance of the model. The intuition behind this approach is that each new weak learner corrects the errors of the previous ensemble, leading to a more accurate final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7a6022-f6fd-4e03-9a2e-2befe26d9c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36fb58f-9477-47c4-8b81-95c8ba4652b9",
   "metadata": {},
   "source": [
    "The Gradient Boosting algorithm builds an ensemble of weak learners in an iterative manner. Here's how it typically works:\n",
    "\n",
    "1. **Initialization**: The algorithm starts with an initial model, which can be a simple model like a decision tree. This model makes predictions based on the input features.\n",
    "\n",
    "2. **Sequential Training**: In each iteration, the algorithm trains a new weak learner to correct the errors made by the ensemble of models constructed so far.\n",
    "\n",
    "3. **Gradient Calculation**: For each iteration, the algorithm calculates the gradients of a loss function with respect to the predictions of the ensemble. These gradients indicate the direction and magnitude of the error that needs to be corrected.\n",
    "\n",
    "4. **Fitting Weak Learner**: The algorithm fits a new weak learner to the gradients calculated in the previous step. This weak learner is trained to predict the residuals or errors of the ensemble.\n",
    "\n",
    "5. **Update Ensemble Predictions**: The predictions of the new weak learner are combined with the predictions of the ensemble so far. This combination is typically achieved by adding the predictions of the new weak learner to the predictions of the ensemble.\n",
    "\n",
    "6. **Repeat**: Steps 3 to 5 are repeated for a fixed number of iterations or until a stopping criterion is met, such as reaching a specified level of accuracy or when adding more weak learners does not significantly improve performance.\n",
    "\n",
    "7. **Final Ensemble Prediction**: The final ensemble prediction is obtained by summing the predictions of all the weak learners. This final prediction represents the aggregated knowledge of the entire ensemble.\n",
    "\n",
    "By iteratively training weak learners to correct the errors of the ensemble and combining their predictions, Gradient Boosting constructs a powerful ensemble model that can capture complex patterns in the data and make accurate predictions. Each weak learner focuses on capturing a different aspect of the data, and their collective strength lies in their ability to complement each other and collectively form a strong predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ae8d649-13c6-4caf-aa94-2cfcbd037877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "# algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016dc9aa-7765-48ac-a9b1-39278f1cfbe8",
   "metadata": {},
   "source": [
    "The mathematical intuition behind the Gradient Boosting algorithm involves several key steps:\n",
    "\n",
    "1. **Initialization**: Start with an initial prediction, typically the average of the target variable for regression problems, or the log-odds for binary classification problems.\n",
    "\n",
    "2. **Compute Residuals**: Calculate the residuals, which are the differences between the actual target values and the predictions made by the current model.\n",
    "\n",
    "3. **Fit a Weak Learner to Residuals**: Train a weak learner, such as a decision tree, to predict the residuals. The goal is to find a model that can capture the errors made by the current ensemble.\n",
    "\n",
    "4. **Update Predictions**: Update the predictions by adding the predictions of the weak learner to the current predictions. This step gradually improves the accuracy of the model by correcting the errors made by the previous ensemble.\n",
    "\n",
    "5. **Repeat Steps 2-4**: Iterate the process by calculating new residuals based on the updated predictions and fitting additional weak learners to these residuals. Each new weak learner focuses on capturing the remaining errors not captured by the previous ones.\n",
    "\n",
    "6. **Combine Predictions**: Combine the predictions of all the weak learners to obtain the final ensemble prediction. This is typically done by summing the predictions of all the weak learners.\n",
    "\n",
    "7. **Regularization**: Optionally, introduce regularization techniques to prevent overfitting, such as limiting the depth of the weak learners or introducing shrinkage parameters to control the contribution of each weak learner.\n",
    "\n",
    "By following these steps iteratively, the Gradient Boosting algorithm constructs an ensemble of weak learners that collectively form a strong predictive model. Each weak learner focuses on capturing different aspects of the data, and their combined predictions produce accurate and robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53a8ec-2263-40d5-af77-cd95e4261285",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
