{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f4b656e-00af-494e-a961-347309d4169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach?\n",
    "# Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317c07f-d713-4814-83b0-31cb18c72c86",
   "metadata": {},
   "source": [
    "Eigenvalues and eigenvectors are mathematical concepts that play a crucial role in various areas, including linear algebra, data analysis, and machine learning. \n",
    "\n",
    "Eigenvalues represent the scalar values that characterize how an operation, such as a linear transformation, scales the corresponding eigenvectors. Eigenvectors are the non-zero vectors that remain in the same direction after the transformation, albeit possibly with a different magnitude. In other words, when a linear transformation is applied to an eigenvector, the resulting vector is a scaled version of the original eigenvector.\n",
    "\n",
    "Eigen-Decomposition is an approach used to decompose a square matrix into its constituent eigenvectors and eigenvalues. Mathematically, for a square matrix \\( A \\), if \\( \\lambda \\) represents an eigenvalue of \\( A \\) and \\( v \\) represents the corresponding eigenvector, then the eigen-decomposition of \\( A \\) is given by:\n",
    "\n",
    "\\[ A v = \\lambda v \\]\n",
    "\n",
    "Here, \\( A \\) is the original matrix, \\( v \\) is the eigenvector, and \\( \\lambda \\) is the eigenvalue. This equation essentially states that when matrix \\( A \\) is multiplied by its eigenvector \\( v \\), the result is a scaled version of the same eigenvector \\( v \\), with the scaling factor being the corresponding eigenvalue \\( \\lambda \\).\n",
    "\n",
    "Let's illustrate this concept with an example:\n",
    "\n",
    "Suppose we have a 2x2 matrix \\( A \\):\n",
    "\n",
    "\\[ A = \\begin{bmatrix} 3 & 1 \\\\ 1 & 3 \\end{bmatrix} \\]\n",
    "\n",
    "To find the eigenvalues and eigenvectors of \\( A \\), we solve the characteristic equation:\n",
    "\n",
    "\\[ \\text{det}(A - \\lambda I) = 0 \\]\n",
    "\n",
    "Where \\( I \\) is the identity matrix and \\( \\lambda \\) is the eigenvalue.\n",
    "\n",
    "Solving for \\( \\lambda \\), we get:\n",
    "\n",
    "\\[ \\text{det}\\left(\\begin{bmatrix} 3-\\lambda & 1 \\\\ 1 & 3-\\lambda \\end{bmatrix}\\right) = (3-\\lambda)^2 - 1 = \\lambda^2 - 6\\lambda + 8 = 0 \\]\n",
    "\n",
    "This equation yields eigenvalues \\( \\lambda_1 = 4 \\) and \\( \\lambda_2 = 2 \\).\n",
    "\n",
    "To find the corresponding eigenvectors, we substitute each eigenvalue back into the equation \\( (A - \\lambda I)v = 0 \\) and solve for \\( v \\).\n",
    "\n",
    "For \\( \\lambda_1 = 4 \\), we have:\n",
    "\n",
    "\\[ \\begin{bmatrix} -1 & 1 \\\\ 1 & -1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\n",
    "\n",
    "Solving this system of linear equations, we find the eigenvector \\( v_1 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\).\n",
    "\n",
    "Similarly, for \\( \\lambda_2 = 2 \\), we have:\n",
    "\n",
    "\\[ \\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\]\n",
    "\n",
    "Which yields another eigenvector \\( v_2 = \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\).\n",
    "\n",
    "So, in summary, the eigenvalues of matrix \\( A \\) are 4 and 2, and the corresponding eigenvectors are \\( \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\) and \\( \\begin{bmatrix} -1 \\\\ 1 \\end{bmatrix} \\), respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a776349-a60f-4754-aa22-47de32b17aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df80e42-d8d1-4d04-93d0-19bb4599e678",
   "metadata": {},
   "source": [
    "Eigen decomposition is a fundamental concept in linear algebra that involves decomposing a square matrix into a set of eigenvectors and eigenvalues. Mathematically, for a square matrix \\( A \\), eigen decomposition can be represented as:\n",
    "\n",
    "\\[ A = Q \\Lambda Q^{-1} \\]\n",
    "\n",
    "Where:\n",
    "- \\( Q \\) is a matrix whose columns are the eigenvectors of \\( A \\),\n",
    "- \\( \\Lambda \\) is a diagonal matrix containing the eigenvalues of \\( A \\) along its diagonal,\n",
    "- \\( Q^{-1} \\) is the inverse of matrix \\( Q \\).\n",
    "\n",
    "In eigen decomposition, we aim to express the original matrix \\( A \\) in terms of its eigenvalues and eigenvectors. The eigenvectors represent the directions along which the transformation represented by matrix \\( A \\) has no stretching or compression, only scaling. The eigenvalues represent the scaling factor for each eigenvector.\n",
    "\n",
    "The significance of eigen decomposition in linear algebra lies in its ability to simplify and reveal important properties of matrices. It provides a way to analyze and understand the behavior of linear transformations represented by matrices. Eigen decomposition is particularly useful in various mathematical and computational applications, including solving systems of linear equations, principal component analysis (PCA), diagonalization of matrices, and solving differential equations.\n",
    "\n",
    "In summary, eigen decomposition allows us to break down a matrix into its fundamental components, providing insights into its behavior and enabling various analytical and computational techniques in linear algebra and related fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef4c8d2-662a-4123-9a3c-e42ab4b0d906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "# Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a801b-84cf-4007-b46c-f1b42ec690bf",
   "metadata": {},
   "source": [
    "A square matrix \\( A \\) is diagonalizable using the Eigen-Decomposition approach if it satisfies the following conditions:\n",
    "\n",
    "1. **The matrix must be square**: Diagonalization is only applicable to square matrices, meaning the number of rows must be equal to the number of columns.\n",
    "\n",
    "2. **The matrix must have \\( n \\) linearly independent eigenvectors**: For an \\( n \\times n \\) matrix, there must exist \\( n \\) linearly independent eigenvectors corresponding to its eigenvalues.\n",
    "\n",
    "3. **The eigenvectors span the entire space**: The set of eigenvectors must span the entire vector space of dimension \\( n \\), meaning they form a basis for the space.\n",
    "\n",
    "Proof:\n",
    "Let \\( A \\) be an \\( n \\times n \\) square matrix. To prove that \\( A \\) is diagonalizable, we need to show that it satisfies the conditions mentioned above.\n",
    "\n",
    "1. Square Matrix: By definition, \\( A \\) is an \\( n \\times n \\) matrix, satisfying the requirement of being square.\n",
    "\n",
    "2. Linearly Independent Eigenvectors: If \\( A \\) has \\( n \\) distinct eigenvalues (which is a necessary condition for diagonalizability), and each eigenvalue has a corresponding eigenvector, then the eigenvectors are linearly independent. This is because each eigenvalue-eigenvector pair represents a distinct direction of transformation, and having \\( n \\) distinct directions ensures linear independence.\n",
    "\n",
    "3. Spanning the Entire Space: Since \\( A \\) has \\( n \\) linearly independent eigenvectors, and each eigenvector corresponds to a distinct eigenvalue, the set of eigenvectors forms a basis for the vector space of dimension \\( n \\). Therefore, they span the entire space.\n",
    "\n",
    "Thus, if a square matrix satisfies these conditions, it can be diagonalized using the Eigen-Decomposition approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49829516-96f7-482b-a2aa-704320f4ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "# How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba5ade3-0796-4d2d-97f9-cb2703bdad1d",
   "metadata": {},
   "source": [
    "The spectral theorem states that for a symmetric matrix, there exists an orthogonal matrix \\( P \\) and a diagonal matrix \\( D \\) such that \\( A = PDP^T \\), where \\( P^T \\) denotes the transpose of \\( P \\). In other words, a symmetric matrix can be diagonalized using an orthogonal matrix.\n",
    "\n",
    "The significance of the spectral theorem in the context of the Eigen-Decomposition approach is that it provides a clear and powerful way to decompose a symmetric matrix into its eigenvalues and eigenvectors. This decomposition allows us to understand the structure and behavior of the matrix in terms of its eigenvalues and eigenvectors.\n",
    "\n",
    "The spectral theorem is closely related to the diagonalizability of a matrix because it essentially states that any symmetric matrix is diagonalizable. This means that for a symmetric matrix \\( A \\), we can find an orthogonal matrix \\( P \\) and a diagonal matrix \\( D \\) such that \\( A = PDP^T \\), where the columns of \\( P \\) are the eigenvectors of \\( A \\) and the diagonal elements of \\( D \\) are the corresponding eigenvalues.\n",
    "\n",
    "For example, consider the following symmetric matrix:\n",
    "\n",
    "\\[ A = \\begin{bmatrix} 4 & 2 \\\\ 2 & 5 \\end{bmatrix} \\]\n",
    "\n",
    "To apply the spectral theorem, we first need to find the eigenvalues and eigenvectors of \\( A \\). After calculating them, suppose we obtain:\n",
    "\n",
    "Eigenvalues:\n",
    "\\[ \\lambda_1 = 1, \\quad \\lambda_2 = 8 \\]\n",
    "\n",
    "Eigenvectors:\n",
    "\\[ v_1 = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix}, \\quad v_2 = \\begin{bmatrix} \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\]\n",
    "\n",
    "Next, we construct the orthogonal matrix \\( P \\) using the eigenvectors:\n",
    "\n",
    "\\[ P = \\begin{bmatrix} -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{bmatrix} \\]\n",
    "\n",
    "Finally, we construct the diagonal matrix \\( D \\) using the eigenvalues:\n",
    "\n",
    "\\[ D = \\begin{bmatrix} 1 & 0 \\\\ 0 & 8 \\end{bmatrix} \\]\n",
    "\n",
    "Now, we can verify that \\( A = PDP^T \\), which confirms the diagonalizability of \\( A \\) and demonstrates the spectral theorem in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90999057-fb6a-4115-a7a3-c51fb35b60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37eb51-17ed-4162-be48-28fdf17c374b",
   "metadata": {},
   "source": [
    "To find the eigenvalues of a matrix \\( A \\), we solve the characteristic equation:\n",
    "\n",
    "\\[ \\text{det}(A - \\lambda I) = 0 \\]\n",
    "\n",
    "where \\( \\lambda \\) is the eigenvalue and \\( I \\) is the identity matrix of the same size as \\( A \\). \n",
    "\n",
    "The eigenvalues represent the scaling factor by which the corresponding eigenvectors are stretched or compressed when \\( A \\) is applied to them. In other words, they indicate the directions along which the transformation represented by the matrix \\( A \\) acts and how much the vectors in those directions are scaled. \n",
    "\n",
    "For example, if \\( A \\) represents a linear transformation in 2D space, the eigenvalues indicate how much the vectors are stretched or compressed along the corresponding eigenvectors. If an eigenvalue is positive, it means the corresponding eigenvector is stretched; if it is negative, the eigenvector is compressed. If an eigenvalue is zero, it means the corresponding eigenvector is a fixed point of the transformation (i.e., it is only scaled but not rotated or sheared). \n",
    "\n",
    "Eigenvalues are essential in various areas of mathematics and engineering, including linear algebra, differential equations, signal processing, and quantum mechanics. They provide valuable insights into the behavior and properties of linear transformations represented by matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b68ccae-9bc4-443b-93c4-48292fc16eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d2892-dd4f-4ad3-94bf-14be2caebd01",
   "metadata": {},
   "source": [
    "Eigenvectors are non-zero vectors that, when operated on by a linear transformation represented by a matrix \\( A \\), are only scaled, but their direction remains unchanged. In other words, an eigenvector \\( v \\) of a matrix \\( A \\) satisfies the equation:\n",
    "\n",
    "\\[ A v = \\lambda v \\]\n",
    "\n",
    "where \\( \\lambda \\) is the corresponding eigenvalue. \n",
    "\n",
    "Eigenvectors are closely related to eigenvalues in that they provide the direction along which the corresponding eigenvalue scales the vector. Each eigenvalue has a corresponding eigenvector, and vice versa. The eigenvalues determine how much the eigenvectors are scaled or stretched/compressed when operated on by the matrix \\( A \\). \n",
    "\n",
    "For example, if \\( A \\) represents a linear transformation in 2D space, the eigenvectors indicate the directions along which the transformation acts, while the eigenvalues indicate how much the vectors in those directions are stretched or compressed. \n",
    "\n",
    "Eigenvectors and eigenvalues play a fundamental role in various mathematical and computational applications, including solving systems of linear equations, analyzing the stability of dynamical systems, and performing dimensionality reduction techniques like Principal Component Analysis (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28d90a01-f727-4d33-9baa-2a70073e8676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1637b5a-a016-4e86-81dd-18740657265c",
   "metadata": {},
   "source": [
    "The geometric interpretation of eigenvectors and eigenvalues provides insight into how linear transformations affect vectors in space.\n",
    "\n",
    "1. **Eigenvectors**: \n",
    "   - Geometrically, eigenvectors represent directions in space that remain unchanged (up to scaling) after a linear transformation is applied.\n",
    "   - For a given linear transformation represented by a matrix \\( A \\), an eigenvector \\( v \\) is a vector that, when transformed by \\( A \\), only changes in magnitude, but not in direction.\n",
    "   - The direction of the eigenvector indicates the axis or direction of stretching or compression caused by the linear transformation.\n",
    "   - Eigenvectors corresponding to distinct eigenvalues are linearly independent and span the space.\n",
    "\n",
    "2. **Eigenvalues**:\n",
    "   - Eigenvalues represent the scaling factor by which the corresponding eigenvectors are stretched or compressed during a linear transformation.\n",
    "   - A larger eigenvalue indicates a greater stretching or compression along the corresponding eigenvector direction, while a smaller eigenvalue indicates less stretching or compression.\n",
    "   - If an eigenvalue is negative, it implies that the corresponding eigenvector is flipped or reversed in direction after the transformation.\n",
    "   - Eigenvalues provide information about the scaling behavior of the linear transformation along different directions in space.\n",
    "\n",
    "In summary, eigenvectors give the directions along which a linear transformation acts, while eigenvalues determine how much the vectors in those directions are scaled. Together, they provide a geometric understanding of how linear transformations deform space. This geometric interpretation is fundamental in various fields, including computer graphics, physics, and data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00931d00-b1e5-4da3-b59d-9a6d8895e374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de3d358-485f-4039-9eb6-83e9066a99ef",
   "metadata": {},
   "source": [
    "Eigen decomposition, also known as eigendecomposition, is a fundamental concept in linear algebra that decomposes a square matrix into its eigenvalues and eigenvectors. This decomposition has numerous real-world applications across various fields:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**: Eigen decomposition is widely used in PCA, a statistical technique for reducing the dimensionality of data. It helps identify the principal components (eigenvectors) that capture the most significant variance in the data.\n",
    "\n",
    "2. **Image Processing**: In image processing, eigen decomposition is employed for tasks such as image compression, feature extraction, and image denoising. Techniques like eigenfaces use eigenvectors to represent and recognize faces.\n",
    "\n",
    "3. **Quantum Mechanics**: Eigen decomposition plays a crucial role in quantum mechanics, particularly in the Schrödinger equation, where it helps find the energy states (eigenvalues) and wavefunctions (eigenvectors) of quantum systems.\n",
    "\n",
    "4. **Graph Theory**: Eigen decomposition is applied in graph theory to analyze the properties of graphs. For instance, the adjacency matrix of a graph can be decomposed to reveal information about its connectivity and structure.\n",
    "\n",
    "5. **Structural Engineering**: Eigen decomposition is utilized in structural engineering for modal analysis, which helps determine the natural frequencies and mode shapes of mechanical systems like bridges and buildings. This information is vital for understanding structural behavior and designing vibration control systems.\n",
    "\n",
    "6. **Signal Processing**: Eigen decomposition is used in signal processing for tasks such as noise reduction, spectral analysis, and filter design. It helps separate signal components and extract relevant information from noisy data.\n",
    "\n",
    "7. **Machine Learning**: Eigen decomposition is a fundamental building block in various machine learning algorithms, including spectral clustering, collaborative filtering, and matrix factorization techniques like Singular Value Decomposition (SVD).\n",
    "\n",
    "8. **Quantum Computing**: Eigen decomposition is essential in quantum computing algorithms, such as Shor's algorithm for integer factorization and quantum phase estimation. It enables the diagonalization of matrices, which is crucial for implementing quantum operations efficiently.\n",
    "\n",
    "These are just a few examples of the diverse range of applications of eigen decomposition in various fields, highlighting its importance in both theoretical and practical contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bf2852f-358b-4ffb-b149-a243fb11d676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de678f-b2fd-4ecc-b94e-c08241bf736e",
   "metadata": {},
   "source": [
    "Yes, a matrix can have more than one set of eigenvectors and eigenvalues under certain conditions. \n",
    "\n",
    "1. **Multiplicity of Eigenvalues**: If a matrix has repeated eigenvalues, also known as eigenvalue multiplicity, it can have multiple linearly independent eigenvectors associated with each repeated eigenvalue. This occurs when the algebraic multiplicity (the number of times an eigenvalue appears as a root of the characteristic polynomial) is less than the geometric multiplicity (the dimension of the eigenspace corresponding to that eigenvalue).\n",
    "\n",
    "2. **Degenerate Matrices**: In some cases, particularly with degenerate or defective matrices, it's possible for a matrix to have fewer linearly independent eigenvectors than its size suggests. This situation arises when there are fewer linearly independent eigenvectors than the dimension of the matrix, leading to a deficiency in the diagonalization process.\n",
    "\n",
    "3. **Non-Diagonalizable Matrices**: Matrices that cannot be diagonalized may have fewer than the expected number of linearly independent eigenvectors. For example, matrices with repeated eigenvalues and insufficient corresponding eigenvectors are not diagonalizable.\n",
    "\n",
    "In summary, while a matrix typically has a unique set of eigenvalues, it can have multiple sets of linearly independent eigenvectors corresponding to these eigenvalues, especially in cases of repeated eigenvalues or non-diagonalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ef737b1-8ba2-4fd5-b95d-ed24d43d330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "# Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282012f0-d5f7-4da4-9f65-f2e371f73077",
   "metadata": {},
   "source": [
    "Eigen-decomposition, or eigendecomposition, is a powerful tool in data analysis and machine learning with various applications. Here are three specific techniques that rely on eigen-decomposition:\n",
    "\n",
    "1. **Principal Component Analysis (PCA)**:\n",
    "   - PCA is a dimensionality reduction technique used to identify patterns and relationships in high-dimensional data by transforming it into a lower-dimensional space. Eigen-decomposition plays a central role in PCA by decomposing the covariance matrix of the data into its eigenvectors and eigenvalues.\n",
    "   - Eigenvectors represent the directions of maximum variance in the data, while eigenvalues indicate the amount of variance explained by each eigenvector.\n",
    "   - By selecting the top eigenvectors associated with the largest eigenvalues, PCA retains the most significant features of the data while reducing its dimensionality. This enables visualization, noise reduction, and feature extraction in various applications such as image processing, genetics, and finance.\n",
    "\n",
    "2. **Spectral Clustering**:\n",
    "   - Spectral clustering is a popular clustering algorithm used to partition data points into meaningful clusters based on the similarity of their feature representations.\n",
    "   - Eigen-decomposition is employed to transform the data into a lower-dimensional space, where clustering is performed more effectively. This transformation is achieved by computing the graph Laplacian matrix of the data, followed by eigen-decomposition of this matrix.\n",
    "   - The eigenvectors corresponding to the smallest eigenvalues of the Laplacian matrix capture the underlying structure of the data, enabling spectral clustering to identify clusters that may have complex shapes or may not be linearly separable in the original feature space.\n",
    "   - Spectral clustering is widely used in image segmentation, community detection in social networks, and document clustering.\n",
    "\n",
    "3. **Eigenfaces for Face Recognition**:\n",
    "   - Eigenfaces is a classic technique for face recognition that relies on eigen-decomposition to represent faces as linear combinations of basis vectors (eigenfaces).\n",
    "   - In this technique, a set of face images is transformed into a high-dimensional vector space using eigen-decomposition of the covariance matrix of the image data.\n",
    "   - The resulting eigenvectors, or eigenfaces, capture the principal components of facial variation in the dataset. By projecting a new face image onto the eigenface space, it can be represented as a weighted sum of eigenfaces.\n",
    "   - Face recognition is then performed by comparing the similarity between the eigenface representations of the query face and the faces in the database.\n",
    "   - Eigenfaces has been widely used in biometrics, security systems, and human-computer interaction applications.\n",
    "\n",
    "These applications illustrate the versatility and utility of eigen-decomposition in data analysis and machine learning, enabling techniques for dimensionality reduction, clustering, and pattern recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384fd016-616a-4afd-b501-7f2039e3cbb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
