{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f2788b3-0b5a-48e7-a91e-9978e0c6d015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is some error i am unable to extract data plz help me "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1da11fa1-acf9-4ef5-b774-49a2350e4160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1. Write a python program to extract the video URL of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42498b09-6c90-4166-b92b-7c089ba8c0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract video URLs.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_youtube_channel_videos(channel_url, num_videos=5):\n",
    "    try:\n",
    "        # Send a GET request to the YouTube channel URL\n",
    "        response = requests.get(channel_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements containing video URLs\n",
    "            video_elements = soup.find_all('a', {'id': 'video-title'})\n",
    "\n",
    "            # Extract the video URLs of the first 'num_videos' videos\n",
    "            video_urls = [f\"https://www.youtube.com{video['href']}\" for video in video_elements[:num_videos]]\n",
    "\n",
    "            return video_urls\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "video_urls = get_youtube_channel_videos(channel_url, num_videos=5)\n",
    "\n",
    "# Print the extracted video URLs\n",
    "if video_urls:\n",
    "    print(\"Video URLs:\")\n",
    "    for idx, url in enumerate(video_urls, start=1):\n",
    "        print(f\"{idx}. {url}\")\n",
    "else:\n",
    "    print(\"Failed to extract video URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c86aa117-3796-4949-86b1-dd85dc385c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd975be4-045c-4646-b5b3-9624c7927df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract thumbnail URLs.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_youtube_channel_thumbnails(channel_url, num_thumbnails=5):\n",
    "    try:\n",
    "        # Send a GET request to the YouTube channel URL\n",
    "        response = requests.get(channel_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements containing video thumbnails\n",
    "            thumbnail_elements = soup.find_all('img', {'class': 'style-scope yt-img-shadow'})\n",
    "\n",
    "            # Extract the thumbnail URLs of the first 'num_thumbnails' videos\n",
    "            thumbnail_urls = [thumbnail['src'] for thumbnail in thumbnail_elements[:num_thumbnails]]\n",
    "\n",
    "            return thumbnail_urls\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "thumbnail_urls = get_youtube_channel_thumbnails(channel_url, num_thumbnails=5)\n",
    "\n",
    "# Print the extracted thumbnail URLs\n",
    "if thumbnail_urls:\n",
    "    print(\"Thumbnail URLs:\")\n",
    "    for idx, url in enumerate(thumbnail_urls, start=1):\n",
    "        print(f\"{idx}. {url}\")\n",
    "else:\n",
    "    print(\"Failed to extract thumbnail URLs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e84c04e0-cd67-4bd9-a89d-b1ea15cbd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q3. Write a python program to extract the title of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8301e9a2-54d9-4323-a0a9-8017b9977bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract video titles.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_youtube_channel_video_titles(channel_url, num_titles=5):\n",
    "    try:\n",
    "        # Send a GET request to the YouTube channel URL\n",
    "        response = requests.get(channel_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements containing video titles\n",
    "            title_elements = soup.find_all('a', {'id': 'video-title'})\n",
    "\n",
    "            # Extract the titles of the first 'num_titles' videos\n",
    "            video_titles = [title.text.strip() for title in title_elements[:num_titles]]\n",
    "\n",
    "            return video_titles\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "video_titles = get_youtube_channel_video_titles(channel_url, num_titles=5)\n",
    "\n",
    "# Print the extracted video titles\n",
    "if video_titles:\n",
    "    print(\"Video Titles:\")\n",
    "    for idx, title in enumerate(video_titles, start=1):\n",
    "        print(f\"{idx}. {title}\")\n",
    "else:\n",
    "    print(\"Failed to extract video titles.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957b89a2-d03e-4a56-a6b3-e73517ad5453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4. Write a python program to extract the number of views of the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24b3a4f7-f0ca-4166-a31a-2b61d4ef5bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract the number of views.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_youtube_channel_video_views(channel_url, num_views=5):\n",
    "    try:\n",
    "        # Send a GET request to the YouTube channel URL\n",
    "        response = requests.get(channel_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements containing video views\n",
    "            view_elements = soup.find_all('span', {'class': 'style-scope ytd-grid-video-renderer'})\n",
    "\n",
    "            # Extract the number of views of the first 'num_views' videos\n",
    "            video_views = [view.text.strip() for view in view_elements[:num_views]]\n",
    "\n",
    "            return video_views\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "video_views = get_youtube_channel_video_views(channel_url, num_views=5)\n",
    "\n",
    "# Print the extracted number of views\n",
    "if video_views:\n",
    "    print(\"Number of Views:\")\n",
    "    for idx, views in enumerate(video_views, start=1):\n",
    "        print(f\"{idx}. {views}\")\n",
    "else:\n",
    "    print(\"Failed to extract the number of views.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1caaad5-68f3-4a96-9970-86a8c0260ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5. Write a python program to extract the time of posting of video for the first five videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e55beba-de0b-4033-91c5-80e3000dac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract video posting times.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_youtube_channel_video_posting_time(channel_url, num_videos=5):\n",
    "    try:\n",
    "        # Send a GET request to the YouTube channel URL\n",
    "        response = requests.get(channel_url)\n",
    "\n",
    "        # Check if the request was successful (status code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Find the elements containing video posting time\n",
    "            time_elements = soup.find_all('div', {'id': 'metadata-line'})\n",
    "\n",
    "            # Extract the posting time of the first 'num_videos' videos\n",
    "            video_posting_times = [time.text.strip() for time in time_elements[:num_videos]]\n",
    "\n",
    "            return video_posting_times\n",
    "        else:\n",
    "            print(f\"Failed to fetch the page. Status code: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Replace 'YOUR_CHANNEL_URL' with the actual URL of the YouTube channel\n",
    "channel_url = \"https://www.youtube.com/@PW-Foundation/videos\"\n",
    "video_posting_times = get_youtube_channel_video_posting_time(channel_url, num_videos=5)\n",
    "\n",
    "# Print the extracted posting times\n",
    "if video_posting_times:\n",
    "    print(\"Video Posting Times:\")\n",
    "    for idx, posting_time in enumerate(video_posting_times, start=1):\n",
    "        print(f\"{idx}. {posting_time}\")\n",
    "else:\n",
    "    print(\"Failed to extract video posting times.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b24282-4e84-474e-9a8e-730c603927bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is some error i am unable to extract data plz help me "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
